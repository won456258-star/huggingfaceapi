# --- 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ ê²½ë¡œ
VECTOR_DB_PATH = "faiss_index"

# --- 2. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í•  í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def load_and_split_document(file_path="my_data.txt"):
    """
    ì£¼ì–´ì§„ ê²½ë¡œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)
    return docs

# --- 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ë¡œë“œ í•¨ìˆ˜ ---
def create_and_store_vector_db():
    """
    ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  FAISSì— ì €ì¥í•˜ê±°ë‚˜, ì´ë¯¸ ìˆë‹¤ë©´ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # Hugging Faceì—ì„œ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # 'jhgan/ko-sroberta-multitask'ëŠ” í•œêµ­ì–´ ë¬¸ë§¥ì„ ì˜ ì´í•´í•˜ëŠ” ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
    print("â³ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    model_name = "jhgan/ko-sroberta-multitask"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # ë§Œì•½ ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´, ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šê³  ë°”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    if os.path.exists(VECTOR_DB_PATH):
        print(f"ğŸ“‚ ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ '{VECTOR_DB_PATH}'ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        vector_db = FAISS.load_local(
            VECTOR_DB_PATH, 
            embeddings,
            # FAISS ë¡œë“œ ì‹œ í•„ìš”í•œ ê¶Œí•œ ì„¤ì •ì…ë‹ˆë‹¤.
            allow_dangerous_deserialization=True 
        )
        print("âœ… ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ!")
        return vector_db

    # íŒŒì¼ì´ ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    print(f"âœ¨ ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    # 1. ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë¶„í• í•©ë‹ˆë‹¤.
    docs = load_and_split_document()
    print(f"ğŸ“„ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆê³ , {len(docs)}ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 2. ë¶„í• ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    print("â³ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ì†Œì— ì €ì¥í•©ë‹ˆë‹¤. (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")
    vector_db = FAISS.from_documents(docs, embeddings)
    
    # 3. ìƒì„±ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    vector_db.save_local(VECTOR_DB_PATH)
    print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì €ì¥ ì™„ë£Œ! ('{VECTOR_DB_PATH}' í´ë”ì— ì €ì¥ë¨)")
    
    return vector_db

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ (í…ŒìŠ¤íŠ¸ìš©) ---
if __name__ == '__main__':
    db = create_and_store_vector_db()
    
    # ë²¡í„° ì €ì¥ì†Œê°€ ì˜ ìƒì„±ë˜ì—ˆëŠ”ì§€ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    query = "ëª¨êµ¬ì¥ì´ ë­ì•¼?"
    results = db.similarity_search(query, k=2) # ê°€ì¥ ìœ ì‚¬í•œ 2ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    
    print(f"\n--- í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ (ì§ˆë¬¸: {query}) ---")
    if results:
        for i, doc in enumerate(results):
            print(f"\n[ê²°ê³¼ {i+1}]")
            print(f"ë‚´ìš©: {doc.page_content[:200]}...") # ë‚´ìš© ì¼ë¶€ë§Œ ì¶œë ¥
    else:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("-----------------------------------")